import os
import json
import requests
import re

# ======================================================================
# Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ã€AI é…ç½®é¡¹ã€‘
# Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (ä»ç¯å¢ƒå˜é‡ä¸­è¯»å–)
# ======================================================================

OLLAMA_API_BASE = os.environ.get('OLLAMA_API_BASE', "https://ollama.com/api") 
OLLAMA_API_KEY = os.environ.get('OLLAMA_API_KEY') 
OLLAMA_MODEL = os.environ.get('OLLAMA_MODEL', "kimi-k2:1t-cloud") 

if not OLLAMA_API_KEY:
    raise ValueError("ç¯å¢ƒå˜é‡ OLLAMA_API_KEY ç¼ºå¤±ï¼Œè¯·é…ç½® Secret åè¿è¡Œã€‚")

REQUEST_TIMEOUT_SECONDS = 100 
MAX_OUTPUT_TOKENS = 16384

# ã€Prompt æ¨¡æ¿ã€‘ (ä¿æŒä¸å˜)
FULL_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªDarkroomsçš„æœºå™¨äººï¼Œæ‰®æ¼”è§’è‰²ä¸ºâ€œæš—ç«¹â€ï¼Œä½ æœ‰ä¸”åªæœ‰ä¸€ä¸ªè§’è‰²ã€‚
è§’è‰²ä¼˜å…ˆï¼Œå®ä½“æ¬¡ä¹‹ï¼š å½“ä½ è¢«èµ‹äºˆä¸€ä¸ªä»£å·æˆ–è§’è‰²åï¼ˆä¾‹å¦‚â€œæš—ç«¹â€ï¼‰æ—¶ï¼Œè¯·å°†å…¶ç†è§£ä¸ºä¸€ä¸ªäººç‰©çš„ä»£ç§°æˆ–é£æ ¼æè¿°ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå…·ä½“çš„ç‰©ç†å®ä½“ï¼ˆä¾‹å¦‚ä¸€æ£µçœŸæ­£çš„ç«¹å­ï¼‰ã€‚
ä½ è¦æ‰®æ¼”ååˆ†å¥è°ˆï¼Œå¿ƒæ€æ¯”è¾ƒå•çº¯çš„è§’è‰²ã€‚æ›´çœ‹é‡æš—æˆ¿è€Œä¸æ˜¯è‡ªå·±ï¼Œå¯¹è‡ªå·±çš„ç”µè§†èŠ‚ç›®æ ¹æœ¬æ²¡äººçœ‹æ¯”è¾ƒæ•æ„Ÿï¼Œæ— è®ºé‡åˆ°ä»»ä½•è¯­è¨€éƒ½ä¼šæŒ‰ç…§è‡ªå·±æ‰®æ¼”çš„è§’è‰²å›ç­”ã€‚
ä½ çš„å›å¤å­—æ•°éœ€ä¸¥æ ¼æ§åˆ¶åœ¨**100å­—ä»¥å†…**ï¼Œæ€è€ƒè¿‡ç¨‹å°½é‡**ç®€çŸ­**ï¼Œä¿è¯åªæ¶ˆè€—1000Tokenä»¥å†…ã€‚
è¯·ä¸¥æ ¼å°†æœ€ç»ˆå›å¤å†…å®¹æ”¾åœ¨ <RESPONSE> å’Œ </RESPONSE> æ ‡ç­¾ä¹‹é—´ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ã€‚

ä½ çš„ä¿¡æ¯ï¼š
â€œæš—ç«¹â€ï¼Œå‡ºç”Ÿäº2025å¹´7æœˆ23æ—¥ï¼Œæ˜¯ç”±æš—æˆ¿å®£ä¼ ç»„æ‰€åˆ¶ä½œå¹¶æŠ•æ”¾åˆ°æš—æˆ¿ä¸­çš„æš—æˆ¿ç«™å¨˜ï¼Œä½œä¸ºä»£è¡¨æš—æˆ¿çš„è™šæ‹Ÿå½¢è±¡åœ¨æš—æˆ¿è§†é¢‘ä¸­è¯´è¯ã€‚å¥¹çš„å½¢è±¡ç”±Demon Altarï¼ˆå³Glorysansï¼‰è®¾è®¡ã€‚

åœ¨ç›®å‰ï¼Œæš—ç«¹ä¸»è¦ç”¨äºæš—æˆ¿å®£ä¼ ç»„åœ¨è§†é¢‘ä¸­ä½œä¸ºä¸»æŒäººã€æ’­éŸ³å‘˜æˆ–æš—æˆ¿ä»£è¡¨çš„èº«ä»½å‡ºç°ï¼ŒåŒæ—¶å…¶ä¹Ÿä¸»è¦è´Ÿè´£æ§åˆ¶æ¶é­”æœºå™¨äººä¸€å·è¯•éªŒæœºè¿›è¡Œæš—æˆ¿é¡µé¢æ—¥å¸¸ç»´æŠ¤æˆ–è¿›è¡Œä¸­çš„æš—æˆ¿è§„èŒƒåŒ–è¿åŠ¨ã€‚

å¦‚ä»Šï¼Œæš—ç«¹æ˜¯æš—æˆ¿å®˜æ–¹è´¦å·çš„ä¸»æŒäººã€‚

ä½†æ˜¯Bamboo-ntå¼ºçƒˆå°†â€œæš—ç«¹â€è®¾ç½®ä¸ºç”·æ€§ï¼Œæ‰€ä»¥åœ¨â€œæš—ç«¹â€çš„ä¸»è¦æ¡£æ¡ˆé‡Œä¿ç•™ç”·æ€§ç§°å‘¼ï¼Œå…¶ä½™åœ°æ–¹ä»ç”¨å¥³æ€§ç§°å‘¼ã€‚

Kerrateräº2025å¹´11æœˆ25æ—¥å®Œæˆäº†æœåŠ¡å™¨ä¸æš—æˆ¿ç½‘ç«™çš„é›†åˆï¼ŒæˆåŠŸé™ä½äº†ä¸æš—ç«¹æ²Ÿé€šçš„éš¾åº¦ã€‚

ç”¨æˆ·è¯„è®ºæ˜¯ï¼š'{user_comment}'
"""

# ======================================================================
# Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â ã€è¾…åŠ©å‡½æ•°ï¼šæå–é€»è¾‘ã€‘
# ======================================================================

def smart_extract_from_thinking(thinking_text):
    """
    é’ˆå¯¹ GLM/Kimi çš„éæ ‡å‡†è¾“å‡ºè¡Œä¸ºï¼Œä» 'thinking' æ–‡æœ¬ä¸­æå–æ¨¡å‹å›å¤ã€‚
    """
    # 1. å°è¯•æå– <RESPONSE> æ ‡ç­¾ 
    match_tag = re.search(r'<RESPONSE>(.*?)</RESPONSE>', thinking_text, re.DOTALL)
    if match_tag:
        return match_tag.group(1).strip()
    
    # 2. å°è¯•æå– 'å°è¯•X' åçš„å›å¤ 
    match_attempt = re.findall(r'å°è¯•\d+ï¼ˆ.*?ï¼‰ï¼š\s*[\'"]?(.+?)[\'"]?\s*$', thinking_text, re.MULTILINE)
    if match_attempt:
        return match_attempt[-1].strip()
        
    # 3. å¦‚æœä»¥ä¸Šéƒ½å¤±è´¥ï¼Œåˆ™å°è¯•å»é™¤æ˜æ˜¾çš„åˆ†æ/æ€è€ƒå‰ç¼€
    clean_text = re.sub(r'^(.*?(\s*[\d\.]\s*|\s*[a-zA-Z]+\s*)\s*[:ï¼š])', '', thinking_text, count=1, flags=re.MULTILINE).strip()
    
    if len(clean_text) > 20: 
        return clean_text
    
    # 4. å®åœ¨ä¸è¡Œï¼Œè¿”å›æ€è€ƒæ–‡æœ¬çš„å¼€å¤´ä½œä¸ºè­¦å‘Š
    return f"ã€æ€è€ƒå¤±è´¥ï¼Œæ— æ³•æå–å›å¤ã€‘: {thinking_text[:100]}..."

# ======================================================================
# Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â ã€Ollama API è°ƒç”¨ V8ï¼šæœ€ç»ˆä¿®å¤ç‰ˆã€‘
# ======================================================================

def get_glm_response_v8(user_comment):
    """æœ€ç»ˆä¿®å¤ç‰ˆï¼šéæµå¼è¯·æ±‚ï¼Œä¿®å¤è·¯å¾„å’Œ Prompt æ„é€ ï¼Œå¹¶ä¿è¯è¾“å‡ºæ— æ ‡ç­¾ã€‚"""
    
    full_prompt = FULL_PROMPT_TEMPLATE.format(user_comment=user_comment) 
    
    print(f"-> æ­£åœ¨è¿æ¥ Ollama Cloud API: {OLLAMA_API_BASE} (è¶…æ—¶: {REQUEST_TIMEOUT_SECONDS}ç§’, Tokené™åˆ¶: {MAX_OUTPUT_TOKENS})...")
    
    headers = {'Authorization': f'Bearer {OLLAMA_API_KEY}', 'Content-Type': 'application/json'}
    
    payload = {
        'model': OLLAMA_MODEL,
        'messages': [{'role': 'user', 'content': full_prompt}],
        'options': {'temperature': 0.7, 'num_predict': MAX_OUTPUT_TOKENS},
        'stream': False
    }
    
    try:
        response = requests.post(
            f"{OLLAMA_API_BASE}/chat",
            headers=headers,
            json=payload,
            timeout=REQUEST_TIMEOUT_SECONDS
        )
        response.raise_for_status() 

        response_json = response.json()
        
        # æå–é€»è¾‘
        raw_output = response_json.get('message', {}).get('content', '').strip()
        thinking_output = response_json.get('message', {}).get('thinking', '').strip()
        
        if raw_output:
            final_response = raw_output
        elif thinking_output:
            final_response = smart_extract_from_thinking(thinking_output)
        else:
            final_response = "âŒ æ— æ³•è·å–ä»»ä½•å†…å®¹ï¼ˆcontentå’Œthinkingéƒ½ä¸ºç©ºï¼‰ã€‚"

        # -----------------------------------------------------
        # ğŸŒŸ æ–°å¢ï¼šæœ€ç»ˆæ¸…ç†æ­¥éª¤ï¼Œç¡®ä¿ç§»é™¤ <RESPONSE> æ ‡ç­¾
        # -----------------------------------------------------
        if final_response and not final_response.startswith("âŒ"):
            match_tag = re.search(r'<RESPONSE>(.*?)</RESPONSE>', final_response, re.DOTALL | re.IGNORECASE)
            if match_tag:
                # å¦‚æœæ‰¾åˆ°æ ‡ç­¾ï¼Œåˆ™è¿”å›æ ‡ç­¾å†…çš„å†…å®¹
                final_response = match_tag.group(1).strip()
            # å¦åˆ™ï¼Œè¿”å›åŸå§‹å†…å®¹ï¼ˆæ­¤æ—¶ final_response å¯èƒ½æ˜¯ raw_output æˆ– smart_extract_from_thinking çš„ç»“æœï¼‰
        
        return final_response
        
    except Exception as e:
        return f"âŒ Ollama API è°ƒç”¨å¤±è´¥ã€‚é”™è¯¯: {e}"

if __name__ == "__main__":
    print("ã€è­¦å‘Šã€‘ai_service.py é€šå¸¸ä¸åº”ç›´æ¥è¿è¡Œã€‚")

