import os
import json
import requests
import re

# ======================================================================
# Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ã€AI é…ç½®é¡¹ã€‘
# Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (ä»ç¯å¢ƒå˜é‡ä¸­è¯»å–)
# ======================================================================

# â— å®‰å…¨è·å–ï¼šä» GitHub Actions çš„ ENV ä¸­è¯»å– API å¯†é’¥
OLLAMA_API_BASE = os.environ.get('OLLAMA_API_BASE', "https://ollama.com/api") 
OLLAMA_API_KEY = os.environ.get('OLLAMA_API_KEY') 
OLLAMA_MODEL = os.environ.get('OLLAMA_MODEL', "kimi-k2:1t-cloud") 

if not OLLAMA_API_KEY:
    # åœ¨ GitHub Actions ä¸­ï¼Œå¦‚æœå¯†é’¥æœªè®¾ç½®ï¼Œè¿™é‡Œä¼šç«‹å³æŠ¥é”™
    raise ValueError("ç¯å¢ƒå˜é‡ OLLAMA_API_KEY ç¼ºå¤±ï¼Œè¯·é…ç½® Secret åè¿è¡Œã€‚")

REQUEST_TIMEOUT_SECONDS = 100 
MAX_OUTPUT_TOKENS = 16384

# ã€Prompt æ¨¡æ¿ä¿æŒä¸å˜ã€‘
FULL_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªDarkroomsçš„æœºå™¨äºº... (æ­¤å¤„çœç•¥ï¼Œä½¿ç”¨æ‚¨çš„å®Œæ•´ Prompt å†…å®¹)
ç”¨æˆ·è¯„è®ºæ˜¯ï¼š'{user_comment}'
"""

# ======================================================================
# Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â ã€è¾…åŠ©å‡½æ•°ï¼šæå–é€»è¾‘ã€‘
# Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (ä¿æŒä¸å˜)
# ======================================================================
# â— è¯·ç¡®ä¿æ‚¨çš„ smart_extract_from_thinking å‡½æ•°åŒ…å«åœ¨è¿™é‡Œã€‚


# ======================================================================
# Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â ã€Ollama API è°ƒç”¨ V8ï¼šæœ€ç»ˆä¿®å¤ç‰ˆã€‘
# ======================================================================

def get_glm_response_v8(user_comment): # ğŸŒŸ ä¿®æ­£ï¼šåªæ¥æ”¶ user_comment
    """æœ€ç»ˆä¿®å¤ç‰ˆï¼šéæµå¼è¯·æ±‚ï¼Œä¿®å¤è·¯å¾„å’Œ Prompt æ„é€ ã€‚"""
    
    # ğŸŒŸ å…³é”®ï¼šåœ¨å‡½æ•°å†…éƒ¨æ„é€ å®Œæ•´çš„ Prompt
    full_prompt = FULL_PROMPT_TEMPLATE.format(user_comment=user_comment) 
    
    print(f"-> æ­£åœ¨è¿æ¥ Ollama Cloud API: {OLLAMA_API_BASE} (è¶…æ—¶: {REQUEST_TIMEOUT_SECONDS}ç§’, Tokené™åˆ¶: {MAX_OUTPUT_TOKENS})...")
    
    headers = {'Authorization': f'Bearer {OLLAMA_API_KEY}', 'Content-Type': 'application/json'}
    
    payload = {
        'model': OLLAMA_MODEL,
        'messages': [{'role': 'user', 'content': full_prompt}], # ä½¿ç”¨æ„é€ å¥½çš„ full_prompt
        'options': {'temperature': 0.7, 'num_predict': MAX_OUTPUT_TOKENS},
        'stream': False
    }
    
    try:
        # è·¯å¾„æ¢å¤ä¸ºç”¨æˆ·æœ¬åœ°å¯è¿è¡Œçš„ /chat
        response = requests.post(
            f"{OLLAMA_API_BASE}/chat", # ğŸŒŸ ä¿®æ­£ API è·¯å¾„
            headers=headers,
            json=payload,
            timeout=REQUEST_TIMEOUT_SECONDS
        )
        response.raise_for_status() 

        response_json = response.json()
        
        # â— æ‚¨çš„è°ƒè¯•å’Œæå–é€»è¾‘ä¿æŒä¸å˜
        raw_output = response_json.get('message', {}).get('content', '').strip()
        thinking_output = response_json.get('message', {}).get('thinking', '').strip()
        
        # ... (æ™ºèƒ½æå–é€»è¾‘)
        if raw_output:
            final_response = raw_output
        elif thinking_output:
            final_response = smart_extract_from_thinking(thinking_output)
        else:
            final_response = "âŒ æ— æ³•è·å–ä»»ä½•å†…å®¹ï¼ˆcontentå’Œthinkingéƒ½ä¸ºç©ºï¼‰ã€‚"

        return final_response
        
    except Exception as e:
        # æ‰“å°è¯¦ç»†é”™è¯¯ï¼Œå¸®åŠ©è°ƒè¯•
        return f"âŒ Ollama API è°ƒç”¨å¤±è´¥ã€‚é”™è¯¯: {e}"

if __name__ == "__main__":
    print("ã€è­¦å‘Šã€‘ai_service.py é€šå¸¸ä¸åº”ç›´æ¥è¿è¡Œã€‚")
